{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lasso Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso Regression, which stands for Least Absolute Shrinkage and Selection Operator, is a type of linear regression that uses shrinkage. Shrinkage here means that the data values are shrunk towards a central point, like the mean. The lasso technique encourages simple, sparse models (i.e., models with fewer parameters). This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination.\n",
    "\n",
    "### Key Features of Lasso Regression:\n",
    "\n",
    "1. **Regularization Term**: The key characteristic of Lasso Regression is that it adds an L1 penalty to the regression model, which is the absolute value of the magnitude of the coefficients. The cost function for Lasso regression is:\n",
    "\n",
    "   $$ \\text{Minimize } \\sum_{i=1}^{n} (y_i - \\sum_{j=1}^{p} x_{ij} \\beta_j)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j| $$\n",
    "\n",
    "   where $ \\lambda $ is the regularization parameter.\n",
    "\n",
    "2. **Feature Selection**: One of the advantages of lasso regression over ridge regression is that it can result in sparse models with few coefficients; some coefficients can become exactly zero and be eliminated from the model. This property is called automatic feature selection and is a form of embedded method.\n",
    "\n",
    "3. **Parameter Tuning**: The strength of the L1 penalty is determined by a parameter, typically denoted as alpha or lambda. Selecting a good value for this parameter is crucial and is typically done using cross-validation.\n",
    "\n",
    "4. **Bias-Variance Tradeoff**: Similar to ridge regression, lasso also manages the bias-variance tradeoff in model training. Increasing the regularization strength increases bias but decreases variance, potentially leading to better generalization on unseen data.\n",
    "\n",
    "5. **Scaling**: Before applying lasso, it is recommended to scale/normalize the data as lasso is sensitive to the scale of input features.\n",
    "\n",
    "### Implementation in Scikit-Learn:\n",
    "\n",
    "Lasso regression can be implemented using the `Lasso` class from Scikit-Learn's `linear_model` module. Here's a basic example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of Lasso: 9.387744740461226\n",
      "MSE of Ridge: 0.05090866185225964\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso, Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Generate some regression data\n",
    "X, y = make_regression(n_samples=1000, n_features=15, noise=0.1, random_state=42)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Lasso regression object\n",
    "lasso = Lasso(alpha=1.0)\n",
    "ridge = Ridge(alpha=1.0)\n",
    "\n",
    "# Fit the model\n",
    "lasso.fit(X_train, y_train)\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lasso = lasso.predict(X_test)\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "# Evaluate the model\n",
    "print(\"MSE of Lasso:\", mean_squared_error(y_test, y_pred_lasso))\n",
    "print(\"MSE of Ridge:\", mean_squared_error(y_test, y_pred_ridge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, `alpha` is the parameter that controls the amount of L1 regularization applied to the model. Fine-tuning `alpha` through techniques like cross-validation is a common practice to find the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Lasso Regression Parameters: {'alpha': 1.0}\n",
      "Best score is 0.9995685234915115\n",
      "Tuned Ridge Regression Parameters: {'alpha': 1.0}\n",
      "Best score is 0.9999981195099323\n"
     ]
    }
   ],
   "source": [
    "# Fine tune alpha value using cv\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "# Create a Lasso regression object\n",
    "lasso = Lasso()\n",
    "\n",
    "# Create a dictionary for the grid search key and values\n",
    "param_grid = {'alpha': np.arange(1, 10, 0.1)}\n",
    "\n",
    "# Use grid search to find the best value for alpha\n",
    "lasso_cv = GridSearchCV(lasso, param_grid, cv=10)\n",
    "\n",
    "# Fit the model\n",
    "lasso_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Lasso Regression Parameters: {}\".format(lasso_cv.best_params_))\n",
    "print(\"Best score is {}\".format(lasso_cv.best_score_))\n",
    "\n",
    "# Create a Ridge regression object\n",
    "ridge = Ridge()\n",
    "\n",
    "# Create a dictionary for the grid search key and values\n",
    "param_grid = {'alpha': np.arange(1, 10, 0.1)}\n",
    "\n",
    "# Use grid search to find the best value for alpha\n",
    "ridge_cv = GridSearchCV(ridge, param_grid, cv=10)\n",
    "\n",
    "# Fit the model\n",
    "ridge_cv.fit(X, y)\n",
    "\n",
    "# Print the tuned parameters and score\n",
    "print(\"Tuned Ridge Regression Parameters: {}\".format(ridge_cv.best_params_))\n",
    "print(\"Best score is {}\".format(ridge_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment Alert: Find out what is L1 or L2 regularization?\n",
    "\n",
    "L1 and L2 regularization are techniques used in machine learning to prevent overfitting and improve the generalization of models. They are regularization methods that add a penalty term to the objective function during the training process.\n",
    "\n",
    "### L1 Regularization (Lasso):\n",
    "\n",
    "L1 regularization, also known as Lasso (Least Absolute Shrinkage and Selection Operator), adds the absolute values of the coefficients as a penalty to the objective function. The L1 regularization term is proportional to the sum of the absolute values of the coefficients:\n",
    "\n",
    "\\[ \\text{L1 regularization term} = \\lambda \\sum_{i=1}^{n} |w_i| \\]\n",
    "\n",
    "- \\( \\lambda \\): Regularization strength, a hyperparameter that controls the degree of regularization.\n",
    "- \\( w_i \\): Coefficients of the model.\n",
    "\n",
    "L1 regularization has a sparsity-inducing property, meaning it tends to force some of the coefficients to exactly zero. This makes it useful for feature selection, as it can lead to sparse models where only a subset of features contributes significantly.\n",
    "\n",
    "### L2 Regularization (Ridge):\n",
    "\n",
    "L2 regularization, also known as Ridge regularization, adds the squared values of the coefficients as a penalty to the objective function. The L2 regularization term is proportional to the sum of the squared values of the coefficients:\n",
    "\n",
    "\\[ \\text{L2 regularization term} = \\lambda \\sum_{i=1}^{n} w_i^2 \\]\n",
    "\n",
    "- \\( \\lambda \\): Regularization strength, a hyperparameter that controls the degree of regularization.\n",
    "- \\( w_i \\): Coefficients of the model.\n",
    "\n",
    "L2 regularization does not enforce sparsity in the coefficients. Instead, it tends to shrink the coefficients toward zero, effectively reducing their impact on the model. L2 regularization is particularly effective when dealing with multicollinearity (highly correlated features).\n",
    "\n",
    "### Key Differences:\n",
    "\n",
    "1. **Effect on Coefficients:**\n",
    "   - L1 regularization tends to produce sparse models with some coefficients exactly equal to zero.\n",
    "   - L2 regularization tends to shrink coefficients toward zero but does not force them to be exactly zero.\n",
    "\n",
    "2. **Feature Selection:**\n",
    "   - L1 regularization is often used for feature selection because of its sparsity-inducing property.\n",
    "   - L2 regularization may not lead to exact feature selection but is effective in handling multicollinearity.\n",
    "\n",
    "3. **Objective Function:**\n",
    "   - L1 regularization adds the absolute values of coefficients to the objective function.\n",
    "   - L2 regularization adds the squared values of coefficients to the objective function.\n",
    "\n",
    "4. **Geometric Interpretation:**\n",
    "   - L1 regularization corresponds to a diamond-shaped penalty region in the coefficient space.\n",
    "   - L2 regularization corresponds to a circular-shaped penalty region.\n",
    "\n",
    "In practice, a combination of L1 and L2 regularization is sometimes used, leading to what is known as Elastic Net regularization. The choice between L1 and L2 regularization often depends on the specific characteristics of the data and the goals of the modeling task. Cross-validation is commonly used to find the optimal values for the regularization hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
